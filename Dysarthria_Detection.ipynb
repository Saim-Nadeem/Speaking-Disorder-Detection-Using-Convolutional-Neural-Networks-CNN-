{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32\n",
      "1    32\n",
      "Name: label, dtype: int64\n",
      "                audio_path  label  \\\n",
      "0    All Non Dys Wav\\1.wav      0   \n",
      "1   All Non Dys Wav\\10.wav      0   \n",
      "2   All Non Dys Wav\\11.wav      0   \n",
      "3   All Non Dys Wav\\12.wav      0   \n",
      "4  All Non Dys Wav\\13,.wav      0   \n",
      "\n",
      "                                       mfcc_features  \\\n",
      "0  [-536.5430908203125, 97.0123519897461, 7.79789...   \n",
      "1  [-435.2642822265625, 80.30461883544922, 3.3133...   \n",
      "2  [-461.0809631347656, 114.24716186523438, 0.501...   \n",
      "3  [-500.19842529296875, 100.60742950439453, -4.4...   \n",
      "4  [-551.3157958984375, 95.65728759765625, -2.064...   \n",
      "\n",
      "                                     chroma_features  \\\n",
      "0  [0.4485311508178711, 0.4648422598838806, 0.439...   \n",
      "1  [0.3792971968650818, 0.39954376220703125, 0.36...   \n",
      "2  [0.37338629364967346, 0.3548053503036499, 0.36...   \n",
      "3  [0.2978111505508423, 0.32145678997039795, 0.37...   \n",
      "4  [0.4390692114830017, 0.4155716300010681, 0.395...   \n",
      "\n",
      "                          spectral_contrast_features  \\\n",
      "0  [22.622229777567934, 10.427688250623467, 12.26...   \n",
      "1  [22.846548567659944, 15.47561084384148, 16.543...   \n",
      "2  [22.022058293108966, 16.089575115889417, 18.24...   \n",
      "3  [21.847833846577657, 15.647255677590282, 17.11...   \n",
      "4  [22.76194128136074, 12.996983653875482, 14.941...   \n",
      "\n",
      "                                    tonnetz_features  \\\n",
      "0  [0.012756848304190162, 0.02331320259618281, 0....   \n",
      "1  [-0.004879234006529842, 0.01881687497594774, -...   \n",
      "2  [0.05994301312627735, 0.07363295838474875, 0.0...   \n",
      "3  [-0.004752239307815576, -0.01990886527441328, ...   \n",
      "4  [-0.012074231142524787, 0.013488332676151807, ...   \n",
      "\n",
      "   zero_crossing_rate_features  \n",
      "0                     0.021896  \n",
      "1                     0.024678  \n",
      "2                     0.023492  \n",
      "3                     0.029626  \n",
      "4                     0.033306  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "import logging\n",
    "import absl.logging\n",
    "\n",
    "# Suppress warnings from librosa and other libraries\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress TensorFlow/absl warnings\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "\n",
    "# Define the path to the audio files\n",
    "normal_audio_dir = \"All Non Dys Wav\"\n",
    "disorder_audio_dir = \"All Dys Wav\"\n",
    "\n",
    "# Create a DataFrame to store audio file paths and labels\n",
    "data = []\n",
    "\n",
    "# Load normal audio files\n",
    "for filename in os.listdir(normal_audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(normal_audio_dir, filename)\n",
    "        data.append((audio_path, 0))\n",
    "\n",
    "# Load disorder audio files\n",
    "for filename in os.listdir(disorder_audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(disorder_audio_dir, filename)\n",
    "        data.append((audio_path, 1))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"audio_path\", \"label\"])\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_mfcc_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfccs, axis=1).tolist()\n",
    "\n",
    "# Function to extract Chroma features\n",
    "def extract_chroma_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    return np.mean(chroma, axis=1).tolist()\n",
    "\n",
    "# Function to extract Spectral Contrast features\n",
    "def extract_spectral_contrast_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    return np.mean(spectral_contrast, axis=1).tolist()\n",
    "\n",
    "# Function to extract Tonnetz features\n",
    "def extract_tonnetz_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    return np.mean(tonnetz, axis=1).tolist()\n",
    "\n",
    "# Function to extract Zero-Crossing Rate features\n",
    "def extract_zero_crossing_rate_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    return np.mean(zero_crossing_rate).tolist()\n",
    "\n",
    "# Apply MFCC feature extraction to each audio file\n",
    "df[\"mfcc_features\"] = df[\"audio_path\"].apply(extract_mfcc_features)\n",
    "\n",
    "# Apply Chroma feature extraction to each audio file\n",
    "df[\"chroma_features\"] = df[\"audio_path\"].apply(extract_chroma_features)\n",
    "\n",
    "# Apply Spectral Contrast feature extraction to each audio file\n",
    "df[\"spectral_contrast_features\"] = df[\"audio_path\"].apply(extract_spectral_contrast_features)\n",
    "\n",
    "# Apply Tonnetz feature extraction to each audio file\n",
    "df[\"tonnetz_features\"] = df[\"audio_path\"].apply(extract_tonnetz_features)\n",
    "\n",
    "# Apply Zero-Crossing Rate feature extraction to each audio file\n",
    "df[\"zero_crossing_rate_features\"] = df[\"audio_path\"].apply(extract_zero_crossing_rate_features)\n",
    "\n",
    "# Verify the balance of the dataset\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Check the first few rows to ensure correct labeling\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Shape: (64, 39)\n",
      "Shape of X before reshaping: (64, 39)\n",
      "Shape of X after reshaping: (64, 39, 1)\n"
     ]
    }
   ],
   "source": [
    "# Ensure the features are combined correctly\n",
    "features = []\n",
    "for index, row in df.iterrows():\n",
    "    combined_features = np.concatenate([\n",
    "        row['mfcc_features'],\n",
    "        row['chroma_features'],\n",
    "        row['spectral_contrast_features'],\n",
    "        row['tonnetz_features'],\n",
    "        [row['zero_crossing_rate_features']]\n",
    "    ])\n",
    "    features.append(combined_features)\n",
    "\n",
    "X = np.array(features)\n",
    "\n",
    "# Debug: Print shape of the combined features\n",
    "print(\"Combined Features Shape:\", X.shape)\n",
    "\n",
    "if X.size == 0:\n",
    "    raise ValueError(\"No valid feature arrays found. Check data preprocessing steps.\")\n",
    "\n",
    "# Ensure all combined features have the same length\n",
    "feature_length = X.shape[1]\n",
    "consistent_length = all(len(feature) == feature_length for feature in X)\n",
    "if not consistent_length:\n",
    "    raise ValueError(\"Inconsistent feature lengths found in the dataset.\")\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "# Normalize features using training data mean and std\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "# Debug: Print shape of X before reshaping\n",
    "print(\"Shape of X before reshaping:\", X.shape)\n",
    "\n",
    "# Reshape features to be compatible with Conv1D\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Debug: Print shape of X after reshaping\n",
    "print(\"Shape of X after reshaping:\", X.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save mean and std for use in prediction\n",
    "np.save('mean.npy', mean)\n",
    "np.save('std.npy', std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 254ms/step - accuracy: 0.6397 - loss: 0.6502 - val_accuracy: 0.6154 - val_loss: 0.6340\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5692 - loss: 0.6240 - val_accuracy: 0.7692 - val_loss: 0.5862\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6893 - loss: 0.5854 - val_accuracy: 0.8462 - val_loss: 0.5457\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7755 - loss: 0.5338 - val_accuracy: 0.8462 - val_loss: 0.5153\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8276 - loss: 0.4794 - val_accuracy: 0.7692 - val_loss: 0.4912\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7729 - loss: 0.4805 - val_accuracy: 0.8462 - val_loss: 0.4561\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8225 - loss: 0.4585 - val_accuracy: 0.8462 - val_loss: 0.4248\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8407 - loss: 0.4362 - val_accuracy: 0.8462 - val_loss: 0.3951\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8903 - loss: 0.3627 - val_accuracy: 0.9231 - val_loss: 0.3713\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8460 - loss: 0.3780 - val_accuracy: 0.9231 - val_loss: 0.3529\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8695 - loss: 0.3265 - val_accuracy: 0.9231 - val_loss: 0.3416\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9007 - loss: 0.3439 - val_accuracy: 0.9231 - val_loss: 0.3264\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9034 - loss: 0.3070 - val_accuracy: 0.9231 - val_loss: 0.2978\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9869 - loss: 0.2170 - val_accuracy: 0.9231 - val_loss: 0.2796\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9634 - loss: 0.2088 - val_accuracy: 0.9231 - val_loss: 0.2607\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9295 - loss: 0.2178 - val_accuracy: 0.9231 - val_loss: 0.2399\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9530 - loss: 0.1852 - val_accuracy: 1.0000 - val_loss: 0.2232\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.1566 - val_accuracy: 1.0000 - val_loss: 0.2071\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.1502 - val_accuracy: 1.0000 - val_loss: 0.1919\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.1337 - val_accuracy: 1.0000 - val_loss: 0.1772\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9765 - loss: 0.1147 - val_accuracy: 1.0000 - val_loss: 0.1630\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0831 - val_accuracy: 1.0000 - val_loss: 0.1525\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.1120 - val_accuracy: 1.0000 - val_loss: 0.1449\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9765 - loss: 0.1155 - val_accuracy: 1.0000 - val_loss: 0.1279\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0861 - val_accuracy: 1.0000 - val_loss: 0.1155\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9869 - loss: 0.0802 - val_accuracy: 1.0000 - val_loss: 0.1079\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0583 - val_accuracy: 1.0000 - val_loss: 0.1009\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0535 - val_accuracy: 1.0000 - val_loss: 0.1009\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0512 - val_accuracy: 1.0000 - val_loss: 0.1051\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0426 - val_accuracy: 1.0000 - val_loss: 0.1053\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0418 - val_accuracy: 1.0000 - val_loss: 0.0976\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0490 - val_accuracy: 1.0000 - val_loss: 0.0858\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0363 - val_accuracy: 1.0000 - val_loss: 0.0835\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9765 - loss: 0.0500 - val_accuracy: 1.0000 - val_loss: 0.0846\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0263 - val_accuracy: 1.0000 - val_loss: 0.0883\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 0.0834\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0291 - val_accuracy: 1.0000 - val_loss: 0.0776\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 0.0784\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0335 - val_accuracy: 1.0000 - val_loss: 0.0690\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 0.0617\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0197 - val_accuracy: 1.0000 - val_loss: 0.0555\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9765 - loss: 0.0428 - val_accuracy: 1.0000 - val_loss: 0.0463\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 0.0497\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0324 - val_accuracy: 1.0000 - val_loss: 0.0450\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0176 - val_accuracy: 1.0000 - val_loss: 0.0465\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9869 - loss: 0.0322 - val_accuracy: 1.0000 - val_loss: 0.0489\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 1.0000 - val_loss: 0.0531\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 1.0000 - val_loss: 0.0603\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0226 - val_accuracy: 1.0000 - val_loss: 0.0701\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0173 - val_accuracy: 1.0000 - val_loss: 0.0889\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Model Implementation\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('dysarthria_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict audio class\n",
    "def predict_audio_class(audio_path, model):\n",
    "    # Extract features from the audio file\n",
    "    mfcc_features = extract_mfcc_features(audio_path)\n",
    "    chroma_features = extract_chroma_features(audio_path)\n",
    "    spectral_contrast_features = extract_spectral_contrast_features(audio_path)\n",
    "    tonnetz_features = extract_tonnetz_features(audio_path)\n",
    "    zero_crossing_rate_features = extract_zero_crossing_rate_features(audio_path)\n",
    "    \n",
    "    # Combine the extracted features\n",
    "    combined_features = np.concatenate([mfcc_features, chroma_features, \n",
    "                                        spectral_contrast_features, tonnetz_features,\n",
    "                                        [zero_crossing_rate_features]])\n",
    "    \n",
    "    # Load mean and std from training phase\n",
    "    mean = np.load('mean.npy')\n",
    "    std = np.load('std.npy')\n",
    "    \n",
    "    # Normalize the features using training mean and std\n",
    "    combined_features_normalized = (combined_features - mean) / std\n",
    "    \n",
    "    # Reshape the features to match the input shape of the model\n",
    "    test_input = combined_features_normalized.reshape(1, combined_features_normalized.shape[0], 1)\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    predictions = model.predict(test_input)\n",
    "    \n",
    "    # Print prediction probabilities for debugging\n",
    "    print(\"Prediction probabilities:\", predictions)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    \n",
    "    return \"Dys\" if predicted_class == 1 else \"Non Dys\"\n",
    "\n",
    "# Load the model for prediction\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('dysarthria_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: Female Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Prediction probabilities: [[0.00128007 0.99871993]]\n",
      "File: MS1.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Prediction probabilities: [[0.01783325 0.98216677]]\n",
      "File: MS2.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Prediction probabilities: [[0.01783325 0.98216677]]\n",
      "File: MS3.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Prediction probabilities: [[0.01076876 0.9892313 ]]\n",
      "File: MS4.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Prediction probabilities: [[0.01716051 0.9828395 ]]\n",
      "File: MS5.wav - Predicted class: Dys\n",
      "Processing folder: Female Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Prediction probabilities: [[0.977368   0.02263196]]\n",
      "File: ZA1.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Prediction probabilities: [[9.9985576e-01 1.4417115e-04]]\n",
      "File: ZA2.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Prediction probabilities: [[0.9892763 0.0107237]]\n",
      "File: ZA3.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Prediction probabilities: [[9.9988067e-01 1.1937322e-04]]\n",
      "File: ZA4.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Prediction probabilities: [[9.9991786e-01 8.2083934e-05]]\n",
      "File: ZA5.wav - Predicted class: Non Dys\n",
      "Processing folder: Male Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Prediction probabilities: [[9.412344e-04 9.990588e-01]]\n",
      "File: AB1.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Prediction probabilities: [[0.00191553 0.99808455]]\n",
      "File: AB2.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Prediction probabilities: [[0.0066125  0.99338746]]\n",
      "File: AB3.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Prediction probabilities: [[0.00113885 0.9988612 ]]\n",
      "File: AB4.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Prediction probabilities: [[0.00818244 0.99181753]]\n",
      "File: AB5.wav - Predicted class: Dys\n",
      "Processing folder: Male Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Prediction probabilities: [[0.7834846  0.21651544]]\n",
      "File: MA1.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Prediction probabilities: [[0.9967091  0.00329084]]\n",
      "File: MA2.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Prediction probabilities: [[0.99876726 0.00123272]]\n",
      "File: MA3.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Prediction probabilities: [[0.99829704 0.00170304]]\n",
      "File: MA4.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Prediction probabilities: [[0.98718137 0.01281872]]\n",
      "File: MA5.wav - Predicted class: Non Dys\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the testing dataset\n",
    "testing_dataset_dir = \"Testing Dataset\"\n",
    "\n",
    "# Iterate through each subfolder and predict the class of each audio file\n",
    "for subfolder in os.listdir(testing_dataset_dir):\n",
    "    subfolder_path = os.path.join(testing_dataset_dir, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        print(f\"Processing folder: {subfolder}\")\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                audio_path = os.path.join(subfolder_path, filename)\n",
    "                predicted_label = predict_audio_class(audio_path, model)\n",
    "                print(f\"File: {filename} - Predicted class: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Prediction probabilities: [[0.00128007 0.99871993]]\n",
      "Predicted class: Dys\n"
     ]
    }
   ],
   "source": [
    "predicted_label = predict_audio_class(\"Testing Dataset\\Female Dys\\MS1.wav\", model)\n",
    "print(\"Predicted class:\", predicted_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
